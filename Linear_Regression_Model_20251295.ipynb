{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDLUFozYafpS"
      },
      "source": [
        "# Linear Regression Model using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-TqFdZLapT4"
      },
      "outputs": [],
      "source": [
        "# Loading and exploring\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Housing_modified.csv'\n",
        "df = pd.read_csv(file_path) #Database loaded\n",
        "\n",
        "print(\"First 5 rows of dataset:\") #Displays first 5 rows\n",
        "print(df.head())\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None) #Shows all rows\n",
        "print(df)\n",
        "\n",
        "print(\"Dataset shape:\")\n",
        "print(df.shape) #Database shape (rows,columns)\n",
        "\n",
        "print(\"Data types:\")\n",
        "print(df.dtypes) #Data types of columns\n",
        "\n",
        "print(\"Missing values in each column:\")\n",
        "print(df.isnull().sum()) #Checks for missing values\n",
        "\n",
        "# Preprocess the Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Housing_modified.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map({'yes': 1, 'no': 0})  # Converts Yes/No to 1/0\n",
        "\n",
        "\n",
        "df = pd.get_dummies(df, columns=['furnishingstatus'], drop_first=True)  # One-hot encoding for 'furnishingstatus'\n",
        "\n",
        "\n",
        "X = df.drop(\"price\", axis=1)   # All columns except price\n",
        "y = df[\"price\"]                # Target variable\n",
        "# This separates Features (X) and Target (y)\n",
        "\n",
        "# Train and Test Split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Training set:\", X_train)\n",
        "print(\"Testing set:\", X_test)\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)\n",
        "\n",
        "# Step 3 - Model Building using sklearn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Housing_modified.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map({'yes': 1, 'no': 0})\n",
        "\n",
        "numerical_cols = [col for col in df.columns if col not in binary_cols + [\"price\"]]\n",
        "\n",
        "df = pd.get_dummies(df, columns=['furnishingstatus'], drop_first=True)\n",
        "\n",
        "X = df.drop(\"price\", axis=1)\n",
        "y = df[\"price\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"Training Set Performance:\")\n",
        "print(\"  Mean Squared Error:\", train_mse)\n",
        "print(\"  R² Score:\", train_r2)\n",
        "\n",
        "print(\"\\nTesting Set Performance:\")\n",
        "print(\"  Mean Squared Error:\", test_mse)\n",
        "print(\"  R² Score:\", test_r2)\n",
        "\n",
        "# Graphs and visual representation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Actual vs Predicted:\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y_test, y_test_pred, alpha=0.7)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2)\n",
        "plt.xlabel(\"Actual Prices\")\n",
        "plt.ylabel(\"Predicted Prices\")\n",
        "plt.title(\"Actual vs Predicted House Prices\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Residual Plot:\n",
        "residuals = y_test - y_test_pred\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(residuals, kde=True, bins=30)\n",
        "plt.xlabel(\"Residuals (Errors)\")\n",
        "plt.title(\"Distribution of Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Predicted vs Residuals:\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.scatter(y_test_pred, residuals, alpha=0.7)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Predicted vs Residuals\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Correlation heatmap for numerical features:\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(df[binary_cols + ['price']].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap (Numerical Features)\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_VcFtKMYz4R"
      },
      "source": [
        "# Linear Regression Model using Normal Equation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0K0yfbTaWS5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Housing_modified.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Preprocessing the data\n",
        "# Convert Yes/No to 1/0\n",
        "yes_no_columns = [\"mainroad\", \"guestroom\", \"basement\",\n",
        "                  \"hotwaterheating\", \"airconditioning\", \"prefarea\"]\n",
        "\n",
        "for col in yes_no_columns:\n",
        "    df[col] = df[col].map({\"yes\": 1, \"no\": 0})\n",
        "\n",
        "# One-hot encode for furnishingstatus:\n",
        "df = pd.get_dummies(df, columns=[\"furnishingstatus\"], drop_first=True)\n",
        "\n",
        "# Ensuring all data is numeric (convert dtype=object → float)\n",
        "df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Defining features (X) and target (y)\n",
        "X = df.drop(\"price\", axis=1).values.astype(float)\n",
        "y = df[\"price\"].values.reshape(-1, 1).astype(float)\n",
        "\n",
        "# Adding intercept column (bias term)\n",
        "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "# Using the Normal Equation -> theta = (X^T X)^(-1) X^T y\n",
        "theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "\n",
        "print(\"Model Parameters (theta):\")\n",
        "print(theta.flatten())\n",
        "\n",
        "# Predictions\n",
        "y_pred = X_b.dot(theta)\n",
        "\n",
        "# Evaluation (R² Score & RMSE)\n",
        "def r2_score(y, y_pred):\n",
        "    ss_res = np.sum((y - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
        "r2 = r2_score(y, y_pred)\n",
        "\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R² Score:\", r2)\n",
        "\n",
        "# Making an Example prediction\n",
        "example = X_b[0].reshape(1, -1)\n",
        "pred_price = example.dot(theta)[0][0]\n",
        "print(\"\\nExample House Features:\", X[0])\n",
        "print(\"Predicted Price:\", pred_price)\n",
        "print(\"Actual Price:\", y[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD1GzpG6YykK"
      },
      "source": [
        "# Linear Regression Model using Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpu4sjDTZCOJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Housing_modified.csv'\n",
        "df = pd.read_csv(file_path) #Loading Dataset\n",
        "\n",
        "binary_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea'] #Convert categorical yes/no columns to binary\n",
        "for col in binary_cols:\n",
        "    df[col] = df[col].map({'yes':1, 'no':0})\n",
        "\n",
        "df = pd.get_dummies(df, columns=['furnishingstatus'], drop_first=False) # One-hot encode furnishingstatus\n",
        "\n",
        "X = df[['area','bedrooms','bathrooms','stories','parking','mainroad','guestroom','basement','hotwaterheating','airconditioning','prefarea','furnishingstatus_furnished','furnishingstatus_semi-furnished','furnishingstatus_unfurnished']].to_numpy(dtype=float)\n",
        "y = df['price'].to_numpy(dtype=float)\n",
        "#Now we need to split data into 80:20 for training and testing:L\n",
        "m = X.shape[0]\n",
        "split_index = int(0.8 * m)\n",
        "\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# Normalising feautures for better accuracy of the model:\n",
        "mu = np.mean(X_train, axis=0)\n",
        "sigma = np.std(X_train, axis=0)\n",
        "X_train = (X_train - mu) / sigma\n",
        "X_test = (X_test - mu) / sigma\n",
        "\n",
        "#Cost Function concept:\n",
        "def compute_cost(X, y, w, b):\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        f_wb = np.dot(X[i], w) + b\n",
        "        cost += (f_wb - y[i])**2\n",
        "    return cost / (2*m)\n",
        "\n",
        "# The concept of gradient function:\n",
        "def compute_gradient(X, y, w, b):\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(n)\n",
        "    dj_db = 0.0\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb = np.dot(X[i], w) + b\n",
        "        error = f_wb - y[i]\n",
        "        dj_dw += error * X[i]\n",
        "        dj_db += error\n",
        "\n",
        "    dj_dw /= m\n",
        "    dj_db /= m\n",
        "    return dj_dw, dj_db\n",
        "# Gradient descent calculation\n",
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
        "    w = copy.deepcopy(w_in)\n",
        "    b = b_in\n",
        "    J_history = []\n",
        "    for i in range(num_iters):\n",
        "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
        "        w -= alpha * dj_dw\n",
        "        b -= alpha * dj_db\n",
        "        if i % 100 == 0:\n",
        "            J_history.append(compute_cost(X, y, w, b))\n",
        "    return w, b, J_history\n",
        "\n",
        "# R2 score\n",
        "def r2_score(y_true, y_pred):\n",
        "    ss_res = np.sum((y_true - y_pred)**2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
        "    return 1 - ss_res/ss_tot\n",
        "\n",
        "# Learning rate tuning\n",
        "learning_rates = [0.0005, 0.001, 0.0015, 0.002, 0.005, 0.006, 0.007,0.008,0.009]\n",
        "iterations = 4000\n",
        "results = {}\n",
        "\n",
        "for alpha in learning_rates:\n",
        "    print(f\"\\nTraining with learning rate: {alpha}\")\n",
        "    initial_w = np.zeros(X_train.shape[1])\n",
        "    initial_b = 0\n",
        "    w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b, alpha, iterations)\n",
        "    y_pred_test = X_test @ w_final + b_final\n",
        "    r2 = r2_score(y_test, y_pred_test)\n",
        "    results[alpha] = {'w': w_final, 'b': b_final, 'J_hist': J_hist, 'r2_test': r2}\n",
        "    print(f\"Test R2: {r2:.4f}\")\n",
        "\n",
        "# Selecting the best learning rate\n",
        "best_alpha = max(results, key=lambda x: results[x]['r2_test'])\n",
        "w_final = results[best_alpha]['w']\n",
        "b_final = results[best_alpha]['b']\n",
        "print(f\"\\nBest learning rate: {best_alpha}, Test R2: {results[best_alpha]['r2_test']:.4f}\")\n",
        "\n",
        "# Ploting all alphas for visual clarity\n",
        "plt.figure(figsize=(8,6))\n",
        "for alpha in learning_rates:\n",
        "    plt.plot(results[alpha]['J_hist'], label=f'alpha={alpha}')\n",
        "plt.xlabel(\"Iterations (x100)\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(\"Cost Convergence for Different Learning Rates\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predicted vs actual\n",
        "y_pred_test = X_test @ w_final + b_final\n",
        "plt.scatter(y_test, y_pred_test, color=\"blue\", alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Prices (Test Data)\")\n",
        "plt.show()\n",
        "\n",
        "#Example:\n",
        "x_house = np.array([1200, 3, 2, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0])\n",
        "x_house_norm = (x_house - mu) / sigma\n",
        "predicted_price = np.dot(x_house_norm, w_final) + b_final\n",
        "print(f\"\\nPredicted price of the house = ${predicted_price:0.0f}\")\n",
        "\n",
        "#Input data to predict the price:\n",
        "\n",
        "area = float(input(\"Enter area of the house: \"))\n",
        "bedrooms = int(input(\"Enter number of bedrooms: \"))\n",
        "bathrooms = int(input(\"Enter number of bathrooms: \"))\n",
        "stories = int(input(\"Enter number of stories: \"))\n",
        "parking = int(input(\"Enter number of parking spots: \"))\n",
        "\n",
        "mainroad = int(input(\"Mainroad? (1 = yes, 0 = no): \"))\n",
        "guestroom = int(input(\"Guestroom? (1 = yes, 0 = no): \"))\n",
        "basement = int(input(\"Basement? (1 = yes, 0 = no): \"))\n",
        "hotwaterheating = int(input(\"Hot water heating? (1 = yes, 0 = no): \"))\n",
        "airconditioning = int(input(\"Air conditioning? (1 = yes, 0 = no): \"))\n",
        "prefarea = int(input(\"Preferred area? (1 = yes, 0 = no): \"))\n",
        "\n",
        "print(\"\\nFurnishing status options:\")\n",
        "print(\"1. Furnished\\n2. Semi-furnished\\n3. Unfurnished\")\n",
        "f_status = int(input(\"Enter furnishing status (1/2/3): \"))\n",
        "\n",
        "furnished = 1 if f_status == 1 else 0\n",
        "semi_furnished = 1 if f_status == 2 else 0\n",
        "unfurnished = 1 if f_status == 3 else 0\n",
        "\n",
        "x_user = np.array([area, bedrooms, bathrooms, stories, parking, mainroad, guestroom, basement, hotwaterheating, airconditioning, prefarea, furnished, semi_furnished, unfurnished], dtype=float)\n",
        "\n",
        "x_user_norm = (x_user - mu) / sigma\n",
        "\n",
        "pred_price = np.dot(x_user_norm, w_final) + b_final\n",
        "print(f\"Predicted Price of the House = ${pred_price:,.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFY1x-pqguP8"
      },
      "source": [
        "#Bonus Challenges\n",
        "\n",
        "#Challenge 1: Implement Linear Regression manually (no sklearn).\n",
        "1. DONE in code_main_using_normal_equation.\n",
        "2. DONE in code_main_using_gradient_descent.\n",
        "\n",
        "#Challenge 2: Try normalising/standardising features and compare results.\n",
        "1. Without normalising (in code_main_using_sklearn), we got: Mean Squared Error (MSE): 1352268306323.821 , R² Score: 0.6608043993998174\n",
        "\n",
        "2. With normalising (in code_main_using_gradient_descent_method), we got: Mean Squared Error (MSE): 1108628115166.75, R2 Score: 0.6813\n",
        "\n",
        "Hence, we can clearly conclude that with normalising, we got a lower MSE (Relatively lower error) and a higher R² score (Higher accuracy of the model).\n",
        "\n",
        "Also, we can conclude that by using normalisation, we can reach minima in relatively lower number of steps..as the contour graph (for example) becomes more uniform and circular.\n",
        "\n",
        "#Challenge 3: Detect and explain any strong correlations between features.\n",
        "Done in last cell of code_main_using_skelearn using correlation heatmap.\n",
        "\n",
        "**Here are the findings:**\n",
        "1. Shows correlation between numerical features and the target (price).\n",
        "airconditioning (0.45), prefarea (0.33), and mainroad (0.30) have stronger positive correlations with price.\n",
        "2. Other features like hotwaterheating have weak/no correlation.\n",
        "This suggests some features contribute more strongly to house price prediction than others.\n",
        "\n",
        "#Challenge 4: Visualise residuals and discuss your findings.\n",
        "Done in last cell of code_main_using_skelearn using distribution of residuals and prediction vs residual graph in the last cell.\n",
        "\n",
        "**Findings: Distribution of residuals:**\n",
        "\n",
        "1. The residuals (errors) are roughly centered around zero.\n",
        "2. The distribution is approximately normal, indicating unbiased predictions.\n",
        "3. However, there are some outliers (large positive/negative errors).\n",
        "4. This suggests the model performs well but has occasional large deviations.\n",
        "\n",
        "**Findings: Prediction vs residual graph:**\n",
        "\n",
        "1. Residuals are scattered randomly around zero, which is a good sign.\n",
        "2. No strong pattern is visible, meaning errors are not dependent on predicted values.\n",
        "3. However, variance increases for higher predicted prices.\n",
        "4. This indicates the model may not be equally accurate across all price ranges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqAK1vzBmISe"
      },
      "source": [
        "# Graphs and visual analysis along with findings and outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iagbMgI0nDxd"
      },
      "outputs": [],
      "source": [
        "# Interpretation of various graphs and conclusions from them:\n",
        "\n",
        "#1. Actual vs Predicted House Prices:\n",
        "from IPython.display import Image, display\n",
        "display(Image(filename=\"Actual_vs_Predicted_prices_graph.png\"))\n",
        "print('''1. The scatter plot shows how well predicted prices align with actual house prices.\n",
        "2. The red diagonal line represents perfect predictions.\n",
        "3. Most points are close to the line, but some deviations indicate errors.\n",
        "4. Overall, the model captures the trend but slightly underestimates higher values and overestimates the lower values, as we expected it to.''')\n",
        "\n",
        "display(Image(filename=\"distribution_of_residuals.png\"))\n",
        "print('''1. The residuals (errors) are roughly centered around zero.\n",
        "2. The distribution is approximately normal, indicating unbiased predictions.\n",
        "3. However, there are some outliers (large positive/negative errors).\n",
        "4. This suggests the model performs well but has occasional large deviations.''')\n",
        "\n",
        "display(Image(filename=\"predicted_vs_residuals.png\"))\n",
        "print('''1. Residuals are scattered randomly around zero, which is a good sign.\n",
        "2. No strong pattern is visible, meaning errors are not dependent on predicted values\n",
        "3. However, variance increases for higher predicted prices.\n",
        "4. This indicates the model may not be equally accurate across all price ranges.''')\n",
        "\n",
        "display(Image(filename=\"correlation_heatmap.png\"))\n",
        "print('''1. Shows correlation between numerical features and the target (price).\n",
        "2. airconditioning (0.45), prefarea (0.33), and mainroad (0.30) have stronger positive correlations with price.\n",
        "3. Other features like hotwaterheating have weak/no correlation.\n",
        "4. This suggests some features contribute more strongly to house price prediction than others.''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ohZlJj3nOxy"
      },
      "source": [
        "# Learning Outcomes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1cU47llnTSV"
      },
      "source": [
        "Through this task, I was able to understand the working of linear regression in depth by implementing it using three different approaches- sklearn’s built-in function, the Normal Equation, and Gradient Descent.\n",
        "Using sklearn helped me see how easily models can be trained with just a few lines of code.\n",
        "The Normal Equation showed me the direct mathematical solution without iterations, which is efficient for smaller datasets.\n",
        "On the other hand, Gradient Descent gave me valuable insight into how optimization works step by step, how the learning rate affects convergence, and why feature normalization is often necessary.\n",
        "I also explored evaluation metrics like MSE and R² score to measure performance.\n",
        "Overall, the task helped me connect theory with practice, compare the strengths and weaknesses of each method, and gain a deeper understanding of how linear regression models actually learn from data.\n",
        "I would like to thank the Coding Club- AI/ML vertical for such an outstanding opportunity.\n",
        "- Rishit, 20251295"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}